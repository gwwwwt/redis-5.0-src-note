# initServer()

> **经过initServerConfig()用默认值初始化Server配置、配置文件解析、命令行参数解析的步骤后，调用initServer()函数根据配置信息初始化网络监听、事件循环以及其它必要的数据结构。**

```c
//server.c
void initServer(void) {
  	int j;

    //注册信号处理函数;  关于各信号的意义参考"man 7 signal"
    //信号处理应用参考《从server.c#initServer学习信号处理函数.md》
  	signal(SIGHUP, SIG_IGN); 
    signal(SIGPIPE, SIG_IGN);
    //下面这行为SIGINT和SIGTERM注册处理函数为sigShutdownHandler,
    //参考下面的第1节
    setupSignalHandlers(); 
  
    //默认syslog功能是关闭的; 参考《2. 初始化默认配置initServerConfig.md》
    //如果用户配置开启syslog功能, 则打开syslog.
    if (server.syslog_enabled) {
        openlog(server.syslog_ident, LOG_PID | LOG_NDELAY | LOG_NOWAIT, 
                server.syslog_facility);
    }
  
    //使用配置信息初始化几个字段值
    server.hz = server.config_hz;
    server.pid = getpid();
    server.current_client = NULL;
    
    //server的集合类型字段初始化为空集合
    server.clients = listCreate(); 
    server.clients_index = raxNew();
    server.clients_to_close = listCreate();
    server.slaves = listCreate();
    server.monitors = listCreate();
    server.clients_pending_write = listCreate();
    server.slaveseldb = -1; /* Force to emit the first SELECT command. */
    server.unblocked_clients = listCreate();
    server.ready_keys = listCreate();
    server.clients_waiting_acks = listCreate();
    server.get_ack_from_slaves = 0;
    server.clients_paused = 0;
    server.system_memory_size = zmalloc_get_memory_size(); //获取内存总量
  
  	/*
  	对象robj的refcout字段存储当前对象的引用次数, 意味着对象是可以共享的; 
  	但是需要注意的是, 只有当对象robj存储的 0～10000 的整数时, robj才会被共享, 
  	且这些共享整数对象的引用计数初始化为INT_MAX, 保证不会被释放;
  	此外, 执行命令时Redis会返回一些字符串回复, 这些字符串对象同样
  	在服务器初始化时创建, 且永远不会尝试释放;
  	所有共享对象都存储在全局结构体变量 shared;
  	*/
  	createSharedObjects();

    /*
    在 initServerConfig()函数中, server.maxclients 默认初始值为10000; 
    此外, redis除与客户端连接的文件描述符之外, 还设置了保留文件描述符数
    量"CONFIG_MIN_RESERVED_FDS=32"用于日志、rdb、aof等;
    所以这里尝试将本进程的文件描述符限制设置为 (10000 + 32);
    
    需要注意的是, 将fd数量限制改为10032时可能由于系统原因无法设置为这么大, 
    所以setrlimit调用可能会报错;
    如果报错的话, 会尝试将 maxclients 值降低16 后, 再重试, 直到设置成功;
    
    所以经过此函数处理之后, server.maxclients 字段值可能不是 10000, 
    而是比这个值小; 
    当然如果降低了 maxclients 字段值, 会输出日志进行提示;
    
    在本机的输出日志中,maximum open files 只是设置到了 4096, 支持在最大
    客户端数是 4064
    */
    adjustOpenFilesLimit();
  
    //重要的创建事件循环; 上面说了Redis设置了保留文件描述符数量
    //CONFIG_MIN_RESERVED_FDS = 32;
    //然而为了提供更安全的保障, 这里Redis又设置
    //"CONFIG_FDSET_INCR = CONFIG_MIN_RESERVED_FDS + 96";
    //所以 事件循环 监听描述符数量最大为 (maxclients + 128)
    server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR);
  
    if (server.el == NULL) {
        serverLog(LL_WARNING, 
                  "Failed creating the event loop. Error message: '%s'",
                  strerror(errno));
        exit(1);
    }
    //为server.db 分配空间
    server.db = zmalloc(sizeof(redisDb)*server.dbnum);
  
  	//创建serversocket/bind/listen, 
    //在默认情况下会绑定IPv4 和 IPv6 两个版本的监听Serversocket;
    //ipfd字段存储 server socket fd; ipfd_count 存储绑定的IP数量;
  	if (server.port != 0 && listenToPort(server.port,server.ipfd,&server.ipfd_count) == C_ERR)
    	exit(1);
  
    // 创建UNIX socket; unixsocket字段指定文件路径; 
    //unixsocketperm字段设置文件权限;
    if (server.unixsocket != NULL) {
        unlink(server.unixsocket); /* don't care if this fails */
        server.sofd = anetUnixServer(server.neterr,server.unixsocket,
            server.unixsocketperm, server.tcp_backlog);
        if (server.sofd == ANET_ERR) {
            serverLog(LL_WARNING, "Opening Unix socket: %s", server.neterr);
            exit(1);
        }
        anetNonBlock(NULL,server.sofd); //这里将unix socket fd设置非阻塞io
    }
    
    //如果既没有TCP socket, 也没有UNIX socket; 则报错
    if (server.ipfd_count == 0 && server.sofd < 0) {
        serverLog(LL_WARNING, "Configured to not listen anywhere, exiting.");
        exit(1);
    }
  
    //初始化数据库
  	server.db = zmalloc(sizeof(redisDb)*server.dbnum);
  	for (j = 0; j < server.dbnum; j++) {
        server.db[j].dict = dictCreate(&dbDictType,NULL);
        server.db[j].expires = dictCreate(&keyptrDictType,NULL);
        server.db[j].blocking_keys = dictCreate(&keylistDictType,NULL);
        server.db[j].ready_keys = dictCreate(&objectKeyPointerValueDictType,NULL);
        server.db[j].watched_keys = dictCreate(&keylistDictType,NULL);
        server.db[j].id = j;
        server.db[j].avg_ttl = 0;
        server.db[j].defrag_later = listCreate();
  	}
    
    evictionPoolAlloc(); /* Initialize the LRU keys pool. ...略 */
    
    //pubsub_channels字段为 (channel => 订阅的 client 列表)
    server.pubsub_channels = dictCreate(&keylistDictType,NULL);
    server.pubsub_patterns = listCreate(); //订阅模式双向链表
    listSetFreeMethod(server.pubsub_patterns,freePubsubPattern);
    listSetMatchMethod(server.pubsub_patterns,listMatchPubsubPattern);
  
    server.cronloops = 0; //cron函数执行次数, 初始为0;
  
    //初始化rdb/aof的进程id, 现在启动时, 将它们初始化为-1
    server.rdb_child_pid = -1;
    server.aof_child_pid = -1;
    
    /* rdb数据输出目标, 是写到磁盘还是网络传输, 初始为0, 未指定;
    	#define RDB_CHILD_TYPE_NONE 0
		#define RDB_CHILD_TYPE_DISK 1     // RDB is written to disk
	    #define RDB_CHILD_TYPE_SOCKET 2   // RDB is written to slave socket
    */
    server.rdb_child_type = RDB_CHILD_TYPE_NONE;
    server.rdb_bgsave_scheduled = 0;  //执行BGSAVE指令时更改为true;
    
    /*
    redisServer中定义了 "int child_info_pipe[2]" 字段; 
    作用应该是 当前进程 和 rdb/aof子进程 的通信管道;
    通信的数据类型是 child_info_data 字段类型;
    	struct {
        int process_type;           // AOF or RDB child?
        size_t cow_size;            // Copy on write size
        unsigned long long magic;   // Magic value to make sure data is valid
    	} child_info_data;
    */
    server.child_info_pipe[0] = -1;
    server.child_info_pipe[1] = -1;
    server.child_info_data.magic = 0;
    
    //将aof_rewrite_buf_blocks字段初始化为空双向链表,listCreate();
    aofRewriteBufferReset();
    server.aof_buf = sdsempty(); //清空aof buf
    server.lastsave = time(NULL); /* At startup we consider the DB saved. */
    server.lastbgsave_try = 0;    /* At startup we never tried to BGSAVE. */
    server.rdb_save_time_last = -1;
    server.rdb_save_time_start = -1;
    server.dirty = 0; //上次save rdb之后是否更新过数据;
    resetServerStats(); //清零server中的状态字段;
    
    //A few stats we don't want to reset: server startup time, and peak mem.
    server.stat_starttime = time(NULL);
    server.stat_peak_memory = 0;
    server.stat_rdb_cow_bytes = 0;
    server.stat_aof_cow_bytes = 0;
    server.cron_malloc_stats.zmalloc_used = 0;
    server.cron_malloc_stats.process_rss = 0;
    server.cron_malloc_stats.allocator_allocated = 0;
    server.cron_malloc_stats.allocator_active = 0;
    server.cron_malloc_stats.allocator_resident = 0;
    server.lastbgsave_status = C_OK;
    server.aof_last_write_status = C_OK;
    server.aof_last_write_errno = 0;
    server.repl_good_slaves_count = 0;
  
  	//注册时间事件, 1毫秒之后执行 serverCron 函数;
  	if (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) {
    	serverPanic("Can't create event loop timers.");
    	exit(1);
  	}
  
  	//注册网络IO事件
  	for (j = 0; j < server.ipfd_count; j++) {
    	if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,acceptTcpHandler,NULL) 
        	== AE_ERR) {
      	serverPanic("Unrecoverable error creating server.ipfd file event.");
    	}
  	}
    
    //注册 UNIX socket事件
    if (server.sofd > 0 && aeCreateFileEvent(server.el,server.sofd,AE_READABLE,
        acceptUnixHandler,NULL) == AE_ERR) {
    	serverPanic("Unrecoverable error creating server.sofd file event.");
    }
		
    //注册 module blocked client 管道事件
  	if (aeCreateFileEvent(server.el, server.module_blocked_pipe[0], AE_READABLE,
        moduleBlockedClientPipeReadable,NULL) == AE_ERR) {
            serverPanic("Error registering the readable event for the module "
                "blocked clients subsystem.");
    }
  
    //如果配置打开aof功能, 则打开 aof_filename 文件; 并在 aof_fd 中保存文件描述符;
    if (server.aof_state == AOF_ON) {
        server.aof_fd = open(server.aof_filename, O_WRONLY|O_APPEND|O_CREAT,0644);
        if (server.aof_fd == -1) {
            serverLog(LL_WARNING, "Can't open the append-only file: %s",strerror(errno));
            exit(1);
        }
    }
    //32位机器上设置内存限制
    if (server.arch_bits == 32 && server.maxmemory == 0) {
        serverLog(LL_WARNING,"Warning: 32 bit instance detected but no memory limit set." 
                  "Setting 3 GB maxmemory limit with 'noeviction' policy now.");
        server.maxmemory = 3072LL*(1024*1024); /* 3 GB */
        server.maxmemory_policy = MAXMEMORY_NO_EVICTION;
    }

    if (server.cluster_enabled) clusterInit(); //集群初始化
  
    replicationScriptCacheInit();
    scriptingInit(1);
    slowlogInit();
    latencyMonitorInit();
    bioInit();
    server.initial_memory_usage = zmalloc_used_memory();
}
```

## 1. 信号处理函数  sigShutdownHandler

> **`setupSignalHandlers()`为`SIGINT`和`SIGTERM`信号注册了对应的处理函数为：`sigShutdownHandler`**
>
> ```c
> void setupSignalHandlers(void) {
>     struct sigaction act;
> 
>     sigemptyset(&act.sa_mask);
>     act.sa_flags = 0;
>     act.sa_handler = sigShutdownHandler;
>     sigaction(SIGTERM, &act, NULL);
>     sigaction(SIGINT, &act, NULL);
> }
> ```

```c
//server.c
static void sigShutdownHandler(int sig) {
    char *msg;
    switch (sig) {
    case SIGINT:
        msg = "Received SIGINT scheduling shutdown...";
        break;
    case SIGTERM:
        msg = "Received SIGTERM scheduling shutdown...";
        break;
    default:
        msg = "Received shutdown signal, scheduling shutdown...";
    };

    //根据下面的代码, 只有先前已经接收到一次中断信号后shutdown_asap才会为true.
    //并且在接收到中断信号后还未能执行serverCron时间事件处理程序. 
    //所以要进入本if代码块只能是第二次收到 SIGINT信号后, 也就代表用户比较急于
    //中断当前的启动过程, 所以不会执行清理工作, 直接退出, 以尽快完成退出流程.
    //但由于没有执行清理工作, exit()参数使用1来告知父进程出错信息.
    if (server.shutdown_asap && sig == SIGINT) {
        serverLogFromHandler(LL_WARNING, "You insist... exiting now.");
        rdbRemoveTempFile(getpid());
        exit(1); 
    } else if (server.loading) {
        exit(0);
    }
    serverLogFromHandler(LL_WARNING, msg);
    //第一次收到中断信号时会将本字段设置为true; 之后的关闭过程推迟到serverCron
    //时间事件处理函数中执行; 
    server.shutdown_asap = 1; 
}
```



## 2. 创建共享对象 createSharedObjects

> **函数中调用的`makeObjectShared()`用于将robj设置为共享对象，逻辑是将对象的`refcount`字段值设置为`INT_MAX`。Redis在修改对象引用数量时，也会先检查是该对象的`refcount`是否是`INT_MAX`，如果是，则认为此对象是共享对象，从而不会对该字段进行修改;**

```c
void createSharedObjects(void) {
    int j;

    shared.crlf = createObject(OBJ_STRING,sdsnew("\r\n"));
    shared.ok = createObject(OBJ_STRING,sdsnew("+OK\r\n"));
    shared.err = createObject(OBJ_STRING,sdsnew("-ERR\r\n"));
    shared.emptybulk = createObject(OBJ_STRING,sdsnew("$0\r\n\r\n"));
    shared.czero = createObject(OBJ_STRING,sdsnew(":0\r\n"));
    shared.cone = createObject(OBJ_STRING,sdsnew(":1\r\n"));
    shared.cnegone = createObject(OBJ_STRING,sdsnew(":-1\r\n"));
    shared.nullbulk = createObject(OBJ_STRING,sdsnew("$-1\r\n"));
    shared.nullmultibulk = createObject(OBJ_STRING,sdsnew("*-1\r\n"));
    shared.emptymultibulk = createObject(OBJ_STRING,sdsnew("*0\r\n"));
    shared.pong = createObject(OBJ_STRING,sdsnew("+PONG\r\n"));
    shared.queued = createObject(OBJ_STRING,sdsnew("+QUEUED\r\n"));
    shared.emptyscan = createObject(OBJ_STRING,sdsnew("*2\r\n$1\r\n0\r\n*0\r\n"));
    shared.wrongtypeerr = createObject(OBJ_STRING,sdsnew(
        "-WRONGTYPE Operation against a key holding the wrong kind of value\r\n"));
    shared.nokeyerr = createObject(OBJ_STRING,sdsnew(
        "-ERR no such key\r\n"));
    shared.syntaxerr = createObject(OBJ_STRING,sdsnew(
        "-ERR syntax error\r\n"));
    shared.sameobjecterr = createObject(OBJ_STRING,sdsnew(
        "-ERR source and destination objects are the same\r\n"));
    shared.outofrangeerr = createObject(OBJ_STRING,sdsnew(
        "-ERR index out of range\r\n"));
    shared.noscripterr = createObject(OBJ_STRING,sdsnew(
        "-NOSCRIPT No matching script. Please use EVAL.\r\n"));
    shared.loadingerr = createObject(OBJ_STRING,sdsnew(
        "-LOADING Redis is loading the dataset in memory\r\n"));
    shared.slowscripterr = createObject(OBJ_STRING,sdsnew(
        "-BUSY Redis is busy running a script. You can only call SCRIPT KILL or SHUTDOWN NOSAVE.\r\n"));
    shared.masterdownerr = createObject(OBJ_STRING,sdsnew(
        "-MASTERDOWN Link with MASTER is down and replica-serve-stale-data is set to 'no'.\r\n"));
    shared.bgsaveerr = createObject(OBJ_STRING,sdsnew(
        "-MISCONF Redis is configured to save RDB snapshots, but it is currently not able to persist on disk. Commands that may modify the data set are disabled, because this instance is configured to report errors during writes if RDB snapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details about the RDB error.\r\n"));
    shared.roslaveerr = createObject(OBJ_STRING,sdsnew(
        "-READONLY You can't write against a read only replica.\r\n"));
    shared.noautherr = createObject(OBJ_STRING,sdsnew(
        "-NOAUTH Authentication required.\r\n"));
    shared.oomerr = createObject(OBJ_STRING,sdsnew(
        "-OOM command not allowed when used memory > 'maxmemory'.\r\n"));
    shared.execaborterr = createObject(OBJ_STRING,sdsnew(
        "-EXECABORT Transaction discarded because of previous errors.\r\n"));
    shared.noreplicaserr = createObject(OBJ_STRING,sdsnew(
        "-NOREPLICAS Not enough good replicas to write.\r\n"));
    shared.busykeyerr = createObject(OBJ_STRING,sdsnew(
        "-BUSYKEY Target key name already exists.\r\n"));
    shared.space = createObject(OBJ_STRING,sdsnew(" "));
    shared.colon = createObject(OBJ_STRING,sdsnew(":"));
    shared.plus = createObject(OBJ_STRING,sdsnew("+"));

    //缓存select db命令字符串
    for (j = 0; j < PROTO_SHARED_SELECT_CMDS; j++) {
        char dictid_str[64];
        int dictid_len;

        dictid_len = ll2string(dictid_str,sizeof(dictid_str),j);
        shared.select[j] = createObject(OBJ_STRING,
            sdscatprintf(sdsempty(),
                "*2\r\n$6\r\nSELECT\r\n$%d\r\n%s\r\n",
                dictid_len, dictid_str));
    }
    shared.messagebulk = createStringObject("$7\r\nmessage\r\n",13);
    shared.pmessagebulk = createStringObject("$8\r\npmessage\r\n",14);
    shared.subscribebulk = createStringObject("$9\r\nsubscribe\r\n",15);
    shared.unsubscribebulk = createStringObject("$11\r\nunsubscribe\r\n",18);
    shared.psubscribebulk = createStringObject("$10\r\npsubscribe\r\n",17);
    shared.punsubscribebulk = createStringObject("$12\r\npunsubscribe\r\n",19);
    shared.del = createStringObject("DEL",3);
    shared.unlink = createStringObject("UNLINK",6);
    shared.rpop = createStringObject("RPOP",4);
    shared.lpop = createStringObject("LPOP",4);
    shared.lpush = createStringObject("LPUSH",5);
    shared.rpoplpush = createStringObject("RPOPLPUSH",9);
    shared.zpopmin = createStringObject("ZPOPMIN",7);
    shared.zpopmax = createStringObject("ZPOPMAX",7);
    
    //缓存0~10000的整数
    for (j = 0; j < OBJ_SHARED_INTEGERS; j++) {
        
        shared.integers[j] = makeObjectShared(createObject(OBJ_STRING,(void*)(long)j));
        shared.integers[j]->encoding = OBJ_ENCODING_INT;
    }
    
    //缓存Redis协议0~32参数数量头和参数字符串长度头字符串
    for (j = 0; j < OBJ_SHARED_BULKHDR_LEN; j++) {
        shared.mbulkhdr[j] = createObject(OBJ_STRING, sdscatprintf(sdsempty(),"*%d\r\n",j));
        shared.bulkhdr[j] = createObject(OBJ_STRING, sdscatprintf(sdsempty(),"$%d\r\n",j));
    }
    /* The following two shared objects, minstring and maxstrings, are not
     * actually used for their value but as a special object meaning
     * respectively the minimum possible string and the maximum possible
     * string in string comparisons for the ZRANGEBYLEX command. */
    shared.minstring = sdsnew("minstring");
    shared.maxstring = sdsnew("maxstring");
}
```

## 3. 创建event loop

> **server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR);**
>
> **参考：《IO多路复用/EPoll文件事件和时间事件.md》这个note**

## 4. 创建IPv4、IPv6、UnixSocket的ServerSocket，监听请求

> **实现参考：《学习笔记/从anet.c学习socket编程.md》**

> **IPv4/IPv6 Socket: **
>
> > `listenToPort(server.port,server.ipfd,&server.ipfd_count);`
>
> **UnixSocket:**
>
> > `server.sofd= anetUnixServer(server.neterr,server.unixsocket,
> >             server.unixsocketperm, server.tcp_backlog);`
>
> **创建的Socket都会调用`anetNonBlock(NULL,fd);`设置非阻塞。**

> ```c
> //server.c 
> //创建server socket以及获取本机ip以及监听端口; 一般情况下, redis会找到2个通配符IP进行绑定监听, ipv4和ipv6; 
> int listenToPort(int port, int *fds, int *count) {
>     int j;
> 
>     /* Force binding of 0.0.0.0 if no bind address is specified, 
>     always entering the loop if j == 0. */
>     if (server.bindaddr_count == 0) server.bindaddr[0] = NULL;
>     for (j = 0; j < server.bindaddr_count || j == 0; j++) {
>         if (server.bindaddr[j] == NULL) {
>             int unsupported = 0;
>             //绑定IPv6 通配符监听
>             fds[*count] = anetTcp6Server(server.neterr,port,NULL, server.tcp_backlog);
>             if (fds[*count] != ANET_ERR) {
>                 anetNonBlock(NULL,fds[*count]);
>                 (*count)++;
>             } else if (errno == EAFNOSUPPORT) {
>                 unsupported++;
>                 serverLog(LL_WARNING,"Not listening to IPv6: unsupproted");
>             }
> 
>             if (*count == 1 || unsupported) {
>                 //绑定IPv4 通配符监听;
>                 fds[*count] = anetTcpServer(server.neterr,port,NULL,server.tcp_backlog);
>                 if (fds[*count] != ANET_ERR) {
>                     anetNonBlock(NULL,fds[*count]);
>                     (*count)++;
>                 } else if (errno == EAFNOSUPPORT) {
>                     unsupported++;
>                     serverLog(LL_WARNING,"Not listening to IPv4: unsupproted");
>                 }
>             }
>             /* Exit the loop if we were able to bind * on IPv4 and 
>             IPv6, otherwise fds[*count] will be ANET_ERR and we'll 
>             print an error and return to the caller with an error. */
>             if (*count + unsupported == 2) break;
>         } else if (strchr(server.bindaddr[j],':')) {
>             /* Bind IPv6 address. */
>             fds[*count] = anetTcp6Server(server.neterr,port,server.bindaddr[j],
>                 server.tcp_backlog);
>         } else {
>             /* Bind IPv4 address. */
>             fds[*count] = anetTcpServer(server.neterr,port,server.bindaddr[j],
>                 server.tcp_backlog);
>         }
>         if (fds[*count] == ANET_ERR) {
>             serverLog(LL_WARNING, 
>                 "Creating Server TCP listening socket %s:%d: %s",
>                 server.bindaddr[j] ? server.bindaddr[j] : "*",
>                 port, server.neterr);
>             return C_ERR;
>         }
>         anetNonBlock(NULL,fds[*count]);
>         (*count)++;
>     }
>     return C_OK;
> }
> ```
>
> 

## 5. 初始化16个空redisDb

> ```c
> for (j = 0; j < server.dbnum; j++) {
>         server.db[j].dict = dictCreate(&dbDictType,NULL);
>         server.db[j].expires = dictCreate(&keyptrDictType,NULL);
>         server.db[j].blocking_keys = dictCreate(&keylistDictType,NULL);
>         server.db[j].ready_keys = dictCreate(&objectKeyPointerValueDictType,NULL);
>         server.db[j].watched_keys = dictCreate(&keylistDictType,NULL);
>         server.db[j].id = j;
>         server.db[j].avg_ttl = 0;
>         server.db[j].defrag_later = listCreate();
> }
> ```

## 6. LRU eviction pool(内存淘汰池??) 分配： evictionPoolAlloc

> **初始化`evict.c#static struct evictionPoolEntry *EvictionPoolLRU`静态变量**
>
> **根据 evictonPoolAlloc 的逻辑：`EvictionPoolLRU`是指向16个`evictionPoolEntry`的数组，应该是每个`evictionPoolEntry`作用于一个redisDb，工作原理未确定，可参考下面的原代码注释...//TODO**

```c
//evict.c
/* LRU approximation algorithm
 *
 * Redis uses an approximation of the LRU algorithm that runs in constant
 * memory. Every time there is a key to expire, we sample N keys (with
 * N very small, usually in around 5) to populate a pool of best keys to
 * evict of M keys (the pool size is defined by EVPOOL_SIZE).
 *
 * The N keys sampled are added in the pool of good keys to expire (the one
 * with an old access time) if they are better than one of the current keys
 * in the pool.
 *
 * After the pool is populated, the best key we have in the pool is expired.
 * However note that we don't remove keys from the pool when they are deleted
 * so the pool may contain keys that no longer exist.
 *
 * When we try to evict a key, and all the entries in the pool don't exist
 * we populate it again. This time we'll be sure that the pool has at least
 * one key that can be evicted, if there is at least one key that can be
 * evicted in the whole database. */
void evictionPoolAlloc(void) {
    struct evictionPoolEntry *ep;
    int j;

    ep = zmalloc(sizeof(*ep)*EVPOOL_SIZE); //16
    for (j = 0; j < EVPOOL_SIZE; j++) {
        ep[j].idle = 0;
        ep[j].key = NULL;
        ep[j].cached = sdsnewlen(NULL,EVPOOL_CACHED_SDS_SIZE); //255
        ep[j].dbid = 0;
    }
    EvictionPoolLRU = ep;
}
```

## 7. pubsub列表初始化

```c
server.pubsub_channels = dictCreate(&keylistDictType,NULL);
server.pubsub_patterns = listCreate(); //订阅模式双向链表
listSetFreeMethod(server.pubsub_patterns,freePubsubPattern);
listSetMatchMethod(server.pubsub_patterns,listMatchPubsubPattern);
```

## 8. 初始化aof rewrite buffer: aofRewriteBufferReset

```c
//aof.c
typedef struct aofrwblock {
    unsigned long used, free;
    char buf[AOF_RW_BUF_BLOCK_SIZE]; //10MB
} aofrwblock;
    
/* This function free the old AOF rewrite buffer if needed, and initialize
 * a fresh new one. It tests for server.aof_rewrite_buf_blocks equal to NULL
 * so can be used for the first initialization as well. */
void aofRewriteBufferReset(void) {
    if (server.aof_rewrite_buf_blocks)
        listRelease(server.aof_rewrite_buf_blocks);

    server.aof_rewrite_buf_blocks = listCreate();
    listSetFreeMethod(server.aof_rewrite_buf_blocks,zfree);
}
```

## 9. 注册fd监听事件及处理函数

> **监听tcp socket可读：**
>
> ```c
> aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,
>                   acceptTcpHandler,NULL) 
> ```
>
> **监听Unix Socket可读：**
>
> ```c
> aeCreateFileEvent(server.el,server.sofd,AE_READABLE,
>         acceptUnixHandler,NULL)
> ```
>
> **监听module pipe[0] 可读：**
>
> ```c
> aeCreateFileEvent(server.el, server.module_blocked_pipe[0], 
>                   AE_READABLE,
>                   moduleBlockedClientPipeReadable,NULL)
> ```

## 10. 打开aof文件

```c
if (server.aof_state == AOF_ON) {
        server.aof_fd = open(server.aof_filename, O_WRONLY|O_APPEND|O_CREAT,
                             0644);
        //...
}
```

## 11. 如果开启集群功能, 初始化集群: clusterInit

> **集群相关，//TODO**

```c
void clusterInit(void) { //cluster.c
    int saveconf = 0;

    server.cluster = zmalloc(sizeof(clusterState));
    server.cluster->myself = NULL;
    server.cluster->currentEpoch = 0;
    server.cluster->state = CLUSTER_FAIL;
    server.cluster->size = 1;
    server.cluster->todo_before_sleep = 0;
    server.cluster->nodes = dictCreate(&clusterNodesDictType,NULL);
    server.cluster->nodes_black_list =
        dictCreate(&clusterNodesBlackListDictType,NULL);
    server.cluster->failover_auth_time = 0;
    server.cluster->failover_auth_count = 0;
    server.cluster->failover_auth_rank = 0;
    server.cluster->failover_auth_epoch = 0;
    server.cluster->cant_failover_reason = CLUSTER_CANT_FAILOVER_NONE;
    server.cluster->lastVoteEpoch = 0;
    for (int i = 0; i < CLUSTERMSG_TYPE_COUNT; i++) {
        server.cluster->stats_bus_messages_sent[i] = 0;
        server.cluster->stats_bus_messages_received[i] = 0;
    }
    server.cluster->stats_pfail_nodes = 0;
    memset(server.cluster->slots,0, sizeof(server.cluster->slots));
    clusterCloseAllSlots();

    /* Lock the cluster config file to make sure every node uses
     * its own nodes.conf. */
    if (clusterLockConfig(server.cluster_configfile) == C_ERR)
        exit(1);

    /* Load or create a new nodes configuration. */
    if (clusterLoadConfig(server.cluster_configfile) == C_ERR) {
        /* No configuration found. We will just use the random name provided
         * by the createClusterNode() function. */
        myself = server.cluster->myself =
            createClusterNode(NULL,CLUSTER_NODE_MYSELF|CLUSTER_NODE_MASTER);
        serverLog(LL_NOTICE,"No cluster configuration found, I'm %.40s",
            myself->name);
        clusterAddNode(myself);
        saveconf = 1;
    }
    if (saveconf) clusterSaveConfigOrDie(1);

    /* We need a listening TCP port for our cluster messaging needs. */
    server.cfd_count = 0;
   
    /* Port sanity check II
     * The other handshake port check is triggered too late to stop
     * us from trying to use a too-high cluster port number. */
    if (server.port > (65535-CLUSTER_PORT_INCR)) {
        serverLog(LL_WARNING, "Redis port number too high. "
                   "Cluster communication port is 10,000 port "
                   "numbers higher than your Redis port. "
                   "Your Redis port number must be "
                   "lower than 55535.");
        exit(1);
    }

    if (listenToPort(server.port+CLUSTER_PORT_INCR,
        server.cfd,&server.cfd_count) == C_ERR)
    {
        exit(1);
    } else {
        int j;

        for (j = 0; j < server.cfd_count; j++) {
            if (aeCreateFileEvent(server.el, server.cfd[j], AE_READABLE,
                clusterAcceptHandler, NULL) == AE_ERR)
                    serverPanic("Unrecoverable error creating Redis Cluster "
                                "file event.");
        }
    }

    /* The slots -> keys map is a radix tree. Initialize it here. */
    server.cluster->slots_to_keys = raxNew();
    memset(server.cluster->slots_keys_count,0,
           sizeof(server.cluster->slots_keys_count));

    /* Set myself->port / cport to my listening ports, we'll just need to
     * discover the IP address via MEET messages. */
    myself->port = server.port;
    myself->cport = server.port+CLUSTER_PORT_INCR;
    if (server.cluster_announce_port)
        myself->port = server.cluster_announce_port;
    if (server.cluster_announce_bus_port)
        myself->cport = server.cluster_announce_bus_port;

    server.cluster->mf_end = 0;
    resetManualFailover();
    clusterUpdateMyselfFlags();
}
```

### 11.1 clusterState结构体

```c
typedef struct clusterState { //cluster.h
    clusterNode *myself;  /* This node */
    uint64_t currentEpoch;
    int state;            /* CLUSTER_OK, CLUSTER_FAIL, ... */
    int size;             /* Num of master nodes with at least one slot */
    dict *nodes;          /* Hash table of name -> clusterNode structures */
    dict *nodes_black_list; /* Nodes we don't re-add for a few seconds. */
    clusterNode *migrating_slots_to[CLUSTER_SLOTS];
    clusterNode *importing_slots_from[CLUSTER_SLOTS];
    clusterNode *slots[CLUSTER_SLOTS]; 
    uint64_t slots_keys_count[CLUSTER_SLOTS];
    rax *slots_to_keys;
    /* The following fields are used to take the slave state on elections. */
    mstime_t failover_auth_time; /* Time of previous or next election. */
    int failover_auth_count;    /* Number of votes received so far. */
    int failover_auth_sent;     /* True if we already asked for votes. */
    int failover_auth_rank;     /* This slave rank for current auth request. */
    uint64_t failover_auth_epoch; /* Epoch of the current election. */
    int cant_failover_reason;   /* Why a slave is currently not able to
                                   failover. See the CANT_FAILOVER_* macros. */
    /* Manual failover state in common. */
    mstime_t mf_end;            /* Manual failover time limit (ms unixtime).
                                   It is zero if there is no MF in progress. */
    /* Manual failover state of master. */
    clusterNode *mf_slave;      /* Slave performing the manual failover. */
    /* Manual failover state of slave. */
    long long mf_master_offset; /* Master offset the slave needs to start MF
                                   or zero if stil not received. */
    int mf_can_start;           /* If non-zero signal that the manual failover
                                   can start requesting masters vote. */
    /* The followign fields are used by masters to take state on elections. */
    uint64_t lastVoteEpoch;     /* Epoch of the last vote granted. */
    int todo_before_sleep; /* Things to do in clusterBeforeSleep(). */
    /* Messages received and sent by type. */
    long long stats_bus_messages_sent[CLUSTERMSG_TYPE_COUNT];
    long long stats_bus_messages_received[CLUSTERMSG_TYPE_COUNT];
    long long stats_pfail_nodes;    /* Number of nodes in PFAIL status,
                                       excluding nodes without address. */
} clusterState;

typedef struct clusterNode {
    mstime_t ctime; /* Node object creation time. */
    char name[CLUSTER_NAMELEN]; /* Node name, hex string, sha1-size */
    int flags;      /* CLUSTER_NODE_... */
    uint64_t configEpoch; /* Last configEpoch observed for this node */
    unsigned char slots[CLUSTER_SLOTS/8]; /* slots handled by this node */
    int numslots;   /* Number of slots handled by this node */
    int numslaves;  /* Number of slave nodes, if this is a master */
    struct clusterNode **slaves; /* pointers to slave nodes */
    struct clusterNode *slaveof; /* pointer to the master node. Note that it
                                    may be NULL even if the node is a slave
                                    if we don't have the master node in our
                                    tables. */
    mstime_t ping_sent;      /* Unix time we sent latest ping */
    mstime_t pong_received;  /* Unix time we received the pong */
    mstime_t fail_time;      /* Unix time when FAIL flag was set */
    mstime_t voted_time;     /* Last time we voted for a slave of this master */
    mstime_t repl_offset_time;  /* Unix time we received offset for this node */
    mstime_t orphaned_time;     /* Starting time of orphaned master condition */
    long long repl_offset;      /* Last known repl offset for this node. */
    char ip[NET_IP_STR_LEN];  /* Latest known IP address of this node */
    int port;                   /* Latest known clients port of this node */
    int cport;                  /* Latest known cluster port of this node. */
    clusterLink *link;          /* TCP/IP link with this node */
    list *fail_reports;         /* List of nodes signaling this as failing */
} clusterNode;
```

## 12. replication script cache init : replicationScriptCacheInit

> **参考下面的原注释信息， replication.c**

```c
/* ----------------------- REPLICATION SCRIPT CACHE --------------------------
 * The goal of this code is to keep track of scripts already sent to every
 * connected slave, in order to be able to replicate EVALSHA as it is without
 * translating it to EVAL every time it is possible.
 *
 * We use a capped collection implemented by a hash table for fast lookup
 * of scripts we can send as EVALSHA, plus a linked list that is used for
 * eviction of the oldest entry when the max number of items is reached.
 *
 * We don't care about taking a different cache for every different slave
 * since to fill the cache again is not very costly, the goal of this code
 * is to avoid that the same big script is trasmitted a big number of times
 * per second wasting bandwidth and processor speed, but it is not a problem
 * if we need to rebuild the cache from scratch from time to time, every used
 * script will need to be transmitted a single time to reappear in the cache.
 * 
 * This is how the system works:
 *      
 * 1) Every time a new slave connects, we flush the whole script cache.
 * 2) We only send as EVALSHA what was sent to the master as EVALSHA, without
 *    trying to convert EVAL into EVALSHA specifically for slaves.
 * 3) Every time we trasmit a script as EVAL to the slaves, we also add the
 *    corresponding SHA1 of the script into the cache as we are sure every
 *    slave knows about the script starting from now.
 * 4) On SCRIPT FLUSH command, we replicate the command to all the slaves
 *    and at the same time flush the script cache.
 * 5) When the last slave disconnects, flush the cache.
 * 6) We handle SCRIPT LOAD as well since that's how scripts are loaded
 *    in the master sometimes.
 */ 
    
/* Initialize the script cache, only called at startup. */
void replicationScriptCacheInit(void) {
    server.repl_scriptcache_size = 10000;
    server.repl_scriptcache_dict = dictCreate(&replScriptCacheDictType,NULL);
    server.repl_scriptcache_fifo = listCreate();
}   
```

## 13. 初始化Lua脚本执行环境  //TODO

> **Redis不仅可以执行Lua代码，还能保证脚本以原子方式执行：在执行脚本时不会执行其他脚本或Redis命令。**

```c
void scriptingInit(int setup) { //scripting.c
    lua_State *lua = lua_open();

    if (setup) {
        server.lua_client = NULL;
        server.lua_caller = NULL;
        server.lua_timedout = 0;
        ldbInit();
    }

    luaLoadLibraries(lua);
    luaRemoveUnsupportedFunctions(lua);
    
    /* Initialize a dictionary we use to map SHAs to scripts.
     * This is useful for replication, as we need to replicate EVALSHA
     * as EVAL, so we need to remember the associated script. */
    server.lua_scripts = dictCreate(&shaScriptObjectDictType,NULL);
    server.lua_scripts_mem = 0;
    
    /* Register the redis commands table and fields */
    lua_newtable(lua); 
    
    /* redis.call */
    lua_pushstring(lua,"call");
    lua_pushcfunction(lua,luaRedisCallCommand);
    lua_settable(lua,-3);

    /* redis.pcall */
    lua_pushstring(lua,"pcall");
    lua_pushcfunction(lua,luaRedisPCallCommand);
    lua_settable(lua,-3);

    /* redis.log and log levels. */
    lua_pushstring(lua,"log");
    lua_pushcfunction(lua,luaLogCommand);
    lua_settable(lua,-3);

    lua_pushstring(lua,"LOG_DEBUG");
    lua_pushnumber(lua,LL_DEBUG);
    lua_settable(lua,-3);

    lua_pushstring(lua,"LOG_VERBOSE");
    lua_pushnumber(lua,LL_VERBOSE);
    lua_settable(lua,-3);

    lua_pushstring(lua,"LOG_NOTICE");
    lua_pushnumber(lua,LL_NOTICE);
    lua_settable(lua,-3);

    lua_pushstring(lua,"LOG_WARNING");
    lua_pushnumber(lua,LL_WARNING);
    
    /* redis.sha1hex */
    lua_pushstring(lua, "sha1hex");
    lua_pushcfunction(lua, luaRedisSha1hexCommand);
    lua_settable(lua, -3);

    /* redis.error_reply and redis.status_reply */
    lua_pushstring(lua, "error_reply");
    lua_pushcfunction(lua, luaRedisErrorReplyCommand);
    lua_settable(lua, -3);
    lua_pushstring(lua, "status_reply");
    lua_pushcfunction(lua, luaRedisStatusReplyCommand);
    lua_settable(lua, -3);

    /* redis.replicate_commands */
    lua_pushstring(lua, "replicate_commands");
    lua_pushcfunction(lua, luaRedisReplicateCommandsCommand);
    lua_settable(lua, -3);

    /* redis.set_repl and associated flags. */
    lua_pushstring(lua,"set_repl");
    lua_pushcfunction(lua,luaRedisSetReplCommand);
    lua_settable(lua,-3);

    lua_pushstring(lua,"REPL_NONE");
    lua_pushnumber(lua,PROPAGATE_NONE);
    lua_settable(lua,-3);

    lua_pushstring(lua,"REPL_AOF");
    lua_pushnumber(lua,PROPAGATE_AOF);
    lua_settable(lua,-3);

    lua_pushstring(lua,"REPL_SLAVE");
    lua_pushnumber(lua,PROPAGATE_REPL);
    lua_settable(lua,-3);

    lua_pushstring(lua,"REPL_REPLICA");
    lua_pushnumber(lua,PROPAGATE_REPL);
    lua_settable(lua,-3);

    lua_pushstring(lua,"REPL_ALL");
    lua_pushnumber(lua,PROPAGATE_AOF|PROPAGATE_REPL);
    lua_settable(lua,-3);

    /* redis.breakpoint */
    lua_pushstring(lua,"breakpoint");
    lua_pushcfunction(lua,luaRedisBreakpointCommand);
    lua_settable(lua,-3);

    /* redis.debug */
    lua_pushstring(lua,"debug");

    /* Finally set the table as 'redis' global var. */
    lua_setglobal(lua,"redis");

    /* Replace math.random and math.randomseed with our implementations. */
    lua_getglobal(lua,"math");

    lua_pushstring(lua,"random");
    lua_pushcfunction(lua,redis_math_random);
    lua_settable(lua,-3);

    lua_pushstring(lua,"randomseed");
    lua_pushcfunction(lua,redis_math_randomseed);
    lua_settable(lua,-3);

    lua_setglobal(lua,"math");

    /* Add a helper function that we use to sort the multi bulk output of non
     * deterministic commands, when containing 'false' elements. */
    {
        char *compare_func =    "function __redis__compare_helper(a,b)\n"
                                "  if a == false then a = '' end\n"
                                "  if b == false then b = '' end\n"
                                "  return a<b\n"
                                "end\n";
         luaL_loadbuffer(lua,compare_func,strlen(compare_func),"@cmp_func_def");
        lua_pcall(lua,0,0,0);
    }

    /* Add a helper function we use for pcall error reporting.
     * Note that when the error is in the C function we want to report the
     * information about the caller, that's what makes sense from the point
     * of view of the user debugging a script. */

    {
        char *errh_func =       "local dbg = debug\n"
                                "function __redis__err__handler(err)\n"
                                "  local i = dbg.getinfo(2,'nSl')\n"
                                "  if i and i.what == 'C' then\n"
                                "    i = dbg.getinfo(3,'nSl')\n"
                                "  end\n"
                                "  if i then\n"
                                "    return i.source .. ':' .. i.currentline .. ': ' .. err\n"
                                "  else\n"
                                "    return err\n"
                                "  end\n"
                                "end\n";
        luaL_loadbuffer(lua,errh_func,strlen(errh_func),"@err_handler_def");
        lua_pcall(lua,0,0,0);
    }

    /* Create the (non connected) client that we use to execute Redis commands
     * inside the Lua interpreter.
     * Note: there is no need to create it again when this function is called
     * by scriptingReset(). */
    if (server.lua_client == NULL) {
        server.lua_client = createClient(-1);
        server.lua_client->flags |= CLIENT_LUA;
    }

    /* Lua beginners often don't use "local", this is likely to introduce
     * subtle bugs in their code. To prevent problems we protect accesses
     * to global variables. */
    scriptingEnableGlobalsProtection(lua);

    server.lua = lua;
}
```

## 14. 初始化后台进程: bioInit

> **//TODO**

```c
/* Initialize the background system, spawning the thread. */
void bioInit(void) { //bio.c
    pthread_attr_t attr;
    pthread_t thread;
    size_t stacksize;
    int j;

    /* Initialization of state vars and objects */
    for (j = 0; j < BIO_NUM_OPS; j++) {
        pthread_mutex_init(&bio_mutex[j],NULL);
        pthread_cond_init(&bio_newjob_cond[j],NULL);
        pthread_cond_init(&bio_step_cond[j],NULL);
        bio_jobs[j] = listCreate();
        bio_pending[j] = 0;
    }

    /* Set the stack size as by default it may be small in some system */
    pthread_attr_init(&attr);
    pthread_attr_getstacksize(&attr,&stacksize);
    if (!stacksize) stacksize = 1; /* The world is full of Solaris Fixes */
    while (stacksize < REDIS_THREAD_STACK_SIZE) stacksize *= 2;
    pthread_attr_setstacksize(&attr, stacksize);

    /* Ready to spawn our threads. We use the single argument the thread
     * function accepts in order to pass the job ID the thread is
     * responsible of. */
    for (j = 0; j < BIO_NUM_OPS; j++) {
        void *arg = (void*)(unsigned long) j;
        if (pthread_create(&thread,&attr,bioProcessBackgroundJobs,arg) != 0) {
            serverLog(LL_WARNING,"Fatal: Can't initialize Background Jobs.");
            exit(1);
        }
        bio_threads[j] = thread;
    }
}
```

